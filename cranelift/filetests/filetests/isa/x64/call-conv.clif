test compile precise-output
target x86_64

function %one_arg(i32) system_v {
    ;; system_v has first param in %rdi, fascall in %rcx
    sig0 = (i32) windows_fastcall
block0(v0: i32):
    call_indirect sig0, v0(v0)
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   subq    %rsp, $32, %rsp
;   virtual_sp_offset_adjust 32
;   movq    %rdi, %rcx
;   movq    %rcx, %rdi
;   call    *%rdi
;   addq    %rsp, $32, %rsp
;   virtual_sp_offset_adjust -32
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %two_args(i32, f32) system_v {
    ;; system_v has params in %rdi, %xmm0, fascall in %rcx, %xmm1
    sig0 = (i32, f32) windows_fastcall
    sig1 = (i32, f32) system_v
block0(v0: i32, v1: f32):
    call_indirect sig0, v0(v0, v1)
    call_indirect sig1, v0(v0, v1)
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rax
;   movdqa  %xmm0, %xmm6
;   subq    %rsp, $32, %rsp
;   virtual_sp_offset_adjust 32
;   movq    %rax, %rcx
;   movdqa  %xmm6, %xmm1
;   movq    %rax, %rdi
;   movdqa  %xmm1, %xmm6
;   call    *%rdi
;   addq    %rsp, $32, %rsp
;   virtual_sp_offset_adjust -32
;   movq    %rdi, %rax
;   movdqa  %xmm6, %xmm0
;   call    *%rax
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %fastcall_to_systemv(i32) windows_fastcall {
    ;; fastcall preserves xmm6+, rbx, rbp, rdi, rsi, r12-r15
    ;; system_v preserves no xmm registers, rbx, rbp, r12-r15
    sig0 = () system_v
block0(v0: i32):
    call_indirect sig0, v0()
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $176, %rsp
;   movq    %rsi, 0(%rsp)
;   movq    %rdi, 8(%rsp)
;   movdqu  %xmm6, 16(%rsp)
;   movdqu  %xmm7, 32(%rsp)
;   movdqu  %xmm8, 48(%rsp)
;   movdqu  %xmm9, 64(%rsp)
;   movdqu  %xmm10, 80(%rsp)
;   movdqu  %xmm11, 96(%rsp)
;   movdqu  %xmm12, 112(%rsp)
;   movdqu  %xmm13, 128(%rsp)
;   movdqu  %xmm14, 144(%rsp)
;   movdqu  %xmm15, 160(%rsp)
; block0:
;   call    *%rcx
;   movq    0(%rsp), %rsi
;   movq    8(%rsp), %rdi
;   movdqu  16(%rsp), %xmm6
;   movdqu  32(%rsp), %xmm7
;   movdqu  48(%rsp), %xmm8
;   movdqu  64(%rsp), %xmm9
;   movdqu  80(%rsp), %xmm10
;   movdqu  96(%rsp), %xmm11
;   movdqu  112(%rsp), %xmm12
;   movdqu  128(%rsp), %xmm13
;   movdqu  144(%rsp), %xmm14
;   movdqu  160(%rsp), %xmm15
;   addq    %rsp, $176, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %many_args(
    ;; rdi, rsi, rdx, rcx, r8, r9,
    i64, i64, i64, i64, i64, i64,

    ;; xmm0-7
    f64, f64, f64, f64, f64, f64, f64, f64,

    ;; stack args
    i64, i32, f32, f64
) system_v {
    sig0 = (
      i64, i64, i64, i64, i64, i64, f64, f64, f64, f64, f64, f64, f64, f64, i64,
      i32, f32, f64
    ) windows_fastcall
block0(
      v0: i64, v1:i64, v2:i64, v3:i64,
      v4:i64, v5:i64,
      v6: f64, v7: f64, v8:f64, v9:f64, v10:f64, v11:f64, v12:f64, v13:f64,
      v14:i64, v15:i32, v16:f32, v17:f64
):
    call_indirect sig0, v0(
      v0, v1, v2, v3,
      v4, v5, v6, v7,
      v8, v9, v10, v11,
      v12, v13, v14, v15,
      v16, v17
    )
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $96, %rsp
;   movq    %rbx, 48(%rsp)
;   movq    %r12, 56(%rsp)
;   movq    %r13, 64(%rsp)
;   movq    %r14, 72(%rsp)
;   movq    %r15, 80(%rsp)
; block0:
;   movq    %rsi, %rbx
;   movq    %rdx, %r14
;   movq    %rcx, %r12
;   movq    %r8, %r13
;   movq    %r9, %r15
;   movq    16(%rbp), %rax
;   movq    24(%rbp), %r11
;   movss   32(%rbp), %xmm9
;   movsd   40(%rbp), %xmm8
;   subq    %rsp, $144, %rsp
;   virtual_sp_offset_adjust 144
;   movq    %rdi, %rcx
;   movq    %rbx, %rdx
;   movq    %r14, %r8
;   movq    %r12, %r9
;   movq    %r13, %rsi
;   movq    %rsi, 32(%rsp)
;   movq    %r15, %rsi
;   movq    %rsi, 40(%rsp)
;   movsd   %xmm0, 48(%rsp)
;   movsd   %xmm1, 56(%rsp)
;   movsd   %xmm2, 64(%rsp)
;   movsd   %xmm3, 72(%rsp)
;   movsd   %xmm4, 80(%rsp)
;   movsd   %xmm5, 88(%rsp)
;   movsd   %xmm6, 96(%rsp)
;   movsd   %xmm7, 104(%rsp)
;   movq    %rax, 112(%rsp)
;   movl    %r11d, 120(%rsp)
;   movss   %xmm9, 128(%rsp)
;   movsd   %xmm8, 136(%rsp)
;   call    *%rdi
;   addq    %rsp, $144, %rsp
;   virtual_sp_offset_adjust -144
;   movq    48(%rsp), %rbx
;   movq    56(%rsp), %r12
;   movq    64(%rsp), %r13
;   movq    72(%rsp), %r14
;   movq    80(%rsp), %r15
;   addq    %rsp, $96, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %many_ints(i64, i64, i64, i64, i64) system_v {
    ;; rdi => rcx
    ;; rsi => rdx
    ;; rdx => r8
    ;; rcx => r9
    ;; r8 => stack
    sig0 = (i64, i64, i64, i64, i64) windows_fastcall
block0(v0: i64, v1:i64, v2:i64, v3:i64, v4:i64):
    call_indirect sig0, v0(v0, v1, v2, v3, v4)
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $16, %rsp
;   movq    %rbx, 0(%rsp)
; block0:
;   movq    %rsi, %r11
;   movq    %rdx, %r9
;   movq    %rcx, %rax
;   movq    %r8, %rbx
;   subq    %rsp, $48, %rsp
;   virtual_sp_offset_adjust 48
;   movq    %rdi, %rcx
;   movq    %r11, %rdx
;   movq    %r9, %r8
;   movq    %rax, %r9
;   movq    %rbx, %r11
;   movq    %r11, 32(%rsp)
;   call    *%rdi
;   addq    %rsp, $48, %rsp
;   virtual_sp_offset_adjust -48
;   movq    0(%rsp), %rbx
;   addq    %rsp, $16, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %many_args2(i32, f32, i64, f64, i32, i32, i32, f32, f64, f32, f64) system_v {
    sig0 = (i32, f32, i64, f64, i32, i32, i32, f32, f64, f32, f64) windows_fastcall
block0(v0: i32, v1: f32, v2: i64, v3: f64, v4: i32, v5: i32, v6: i32, v7: f32, v8: f64, v9: f32, v10: f64):
    call_indirect sig0, v0(v0, v1, v2, v3, v4, v5, v6, v7, v8, v9, v10)
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movdqa  %xmm0, %xmm6
;   movq    %rsi, %rax
;   movdqa  %xmm1, %xmm14
;   movq    %rcx, %r11
;   movq    %r8, %r9
;   movdqa  %xmm3, %xmm10
;   subq    %rsp, $96, %rsp
;   virtual_sp_offset_adjust 96
;   movq    %rdi, %rcx
;   movdqa  %xmm6, %xmm1
;   movq    %rax, %r8
;   movdqa  %xmm14, %xmm3
;   movl    %edx, 32(%rsp)
;   movq    %r11, %rdx
;   movl    %edx, 40(%rsp)
;   movq    %r9, %rax
;   movl    %eax, 48(%rsp)
;   movss   %xmm2, 56(%rsp)
;   movdqa  %xmm10, %xmm2
;   movsd   %xmm2, 64(%rsp)
;   movss   %xmm4, 72(%rsp)
;   movsd   %xmm5, 80(%rsp)
;   call    *%rdi
;   addq    %rsp, $96, %rsp
;   virtual_sp_offset_adjust -96
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %wasmtime_mix1(i32) wasmtime_system_v {
    sig0 = (i32) system_v
block0(v0: i32):
    call_indirect sig0, v0(v0)
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rcx
;   call    *%rcx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %wasmtime_mix2(i32) system_v {
    sig0 = (i32) wasmtime_system_v
block0(v0: i32):
    call_indirect sig0, v0(v0)
    return
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movq    %rdi, %rcx
;   call    *%rcx
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %wasmtime_mix2() -> i32, i32 system_v {
    sig0 = () -> i32, i32 wasmtime_system_v
block0:
    v2 = iconst.i32 1
    v0, v1 = call_indirect sig0, v2()
    return v0, v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
; block0:
;   movl    $1, %edx
;   subq    %rsp, $16, %rsp
;   virtual_sp_offset_adjust 16
;   lea     0(%rsp), %rdi
;   call    *%rdx
;   movq    0(%rsp), %rdx
;   addq    %rsp, $16, %rsp
;   virtual_sp_offset_adjust -16
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %wasmtime_mix3() -> i32, i32 wasmtime_system_v {
    sig0 = () -> i32, i32 system_v
block0:
    v2 = iconst.i32 1
    v0, v1 = call_indirect sig0, v2()
    return v0, v1
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $16, %rsp
;   movq    %r13, 0(%rsp)
; block0:
;   movq    %rdi, %r13
;   movl    $1, %r9d
;   call    *%r9
;   movq    %r13, %rdi
;   movl    %edx, 0(%rdi)
;   movq    0(%rsp), %r13
;   addq    %rsp, $16, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %wasmtime_mix4() -> i32, i64, i32 wasmtime_system_v {
    sig0 = () -> i32, i64, i32 system_v
block0:
    v3 = iconst.i32 1
    v0, v1, v2 = call_indirect sig0, v3()
    return v0, v1, v2
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $16, %rsp
;   movq    %rbx, 0(%rsp)
; block0:
;   movq    %rdi, %rbx
;   movl    $1, %esi
;   subq    %rsp, $16, %rsp
;   virtual_sp_offset_adjust 16
;   lea     0(%rsp), %rdi
;   call    *%rsi
;   movq    0(%rsp), %rcx
;   addq    %rsp, $16, %rsp
;   virtual_sp_offset_adjust -16
;   movq    %rbx, %rdi
;   movq    %rdx, 0(%rdi)
;   movl    %ecx, 8(%rdi)
;   movq    0(%rsp), %rbx
;   addq    %rsp, $16, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %wasmtime_mix5() -> f32, i64, i32, f32 wasmtime_system_v {
    sig0 = () -> f32, i64, i32, f32 system_v
block0:
    v5 = iconst.i32 1
    v0, v1, v2, v3 = call_indirect sig0, v5()
    return v0, v1, v2, v3
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $16, %rsp
;   movq    %r12, 0(%rsp)
; block0:
;   movq    %rdi, %r12
;   movl    $1, %eax
;   call    *%rax
;   movq    %r12, %rdi
;   movq    %rax, 0(%rdi)
;   movl    %edx, 8(%rdi)
;   movss   %xmm1, 12(%rdi)
;   movq    0(%rsp), %r12
;   addq    %rsp, $16, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

function %wasmtime_mix6(f32, i64, i32, f32) -> f32, i64, i32, f32 wasmtime_system_v {
    sig0 = (f32, i64, i32, f32) -> f32, i64, i32, f32 system_v
block0(v0: f32, v1: i64, v2: i32, v3: f32):
    v4 = iconst.i32 1
    v5, v6, v7, v8 = call_indirect sig0, v4(v0, v1, v2, v3)
    return v5, v6, v7, v8
}

;   pushq   %rbp
;   movq    %rsp, %rbp
;   subq    %rsp, $16, %rsp
;   movq    %rbx, 0(%rsp)
; block0:
;   movq    %rdx, %rbx
;   movl    $1, %eax
;   call    *%rax
;   movq    %rbx, %rcx
;   movq    %rax, 0(%rcx)
;   movl    %edx, 8(%rcx)
;   movss   %xmm1, 12(%rcx)
;   movq    0(%rsp), %rbx
;   addq    %rsp, $16, %rsp
;   movq    %rbp, %rsp
;   popq    %rbp
;   ret

