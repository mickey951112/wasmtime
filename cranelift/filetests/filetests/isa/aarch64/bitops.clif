test compile precise-output
set unwind_info=false
target aarch64

function %a(i8) -> i8 {
block0(v0: i8):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit w3, w0
;   lsr w0, w3, #24
;   ret

function %a(i16) -> i16 {
block0(v0: i16):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit w3, w0
;   lsr w0, w3, #16
;   ret

function %a(i32) -> i32 {
block0(v0: i32):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit w0, w0
;   ret

function %a(i64) -> i64 {
block0(v0: i64):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit x0, x0
;   ret

function %a(i128) -> i128 {
block0(v0: i128):
    v1 = bitrev v0
    return v1
}

; block0:
;   rbit x6, x0
;   rbit x0, x1
;   mov x1, x6
;   ret

function %b(i8) -> i8 {
block0(v0: i8):
    v1 = clz v0
    return v1
}

; block0:
;   uxtb w3, w0
;   clz w5, w3
;   sub w0, w5, #24
;   ret

function %b(i16) -> i16 {
block0(v0: i16):
    v1 = clz v0
    return v1
}

; block0:
;   uxth w3, w0
;   clz w5, w3
;   sub w0, w5, #16
;   ret

function %b(i32) -> i32 {
block0(v0: i32):
    v1 = clz v0
    return v1
}

; block0:
;   clz w0, w0
;   ret

function %b(i64) -> i64 {
block0(v0: i64):
    v1 = clz v0
    return v1
}

; block0:
;   clz x0, x0
;   ret

function %b(i128) -> i128 {
block0(v0: i128):
    v1 = clz v0
    return v1
}

; block0:
;   clz x6, x1
;   clz x8, x0
;   lsr x10, x6, #6
;   madd x0, x8, x10, x6
;   movz x1, #0
;   ret

function %c(i8) -> i8 {
block0(v0: i8):
    v1 = cls v0
    return v1
}

; block0:
;   sxtb w3, w0
;   cls w5, w3
;   sub w0, w5, #24
;   ret

function %c(i16) -> i16 {
block0(v0: i16):
    v1 = cls v0
    return v1
}

; block0:
;   sxth w3, w0
;   cls w5, w3
;   sub w0, w5, #16
;   ret

function %c(i32) -> i32 {
block0(v0: i32):
    v1 = cls v0
    return v1
}

; block0:
;   cls w0, w0
;   ret

function %c(i64) -> i64 {
block0(v0: i64):
    v1 = cls v0
    return v1
}

; block0:
;   cls x0, x0
;   ret

function %c(i128) -> i128 {
block0(v0: i128):
    v1 = cls v0
    return v1
}

; block0:
;   cls x6, x0
;   cls x8, x1
;   eon x10, x1, x0
;   lsr x12, x10, #63
;   madd x14, x6, x12, x12
;   subs xzr, x8, #63
;   csel x1, x14, xzr, eq
;   add x0, x1, x8
;   movz x1, #0
;   ret

function %d(i8) -> i8 {
block0(v0: i8):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit w3, w0
;   orr w5, w3, #8388608
;   clz w0, w5
;   ret

function %d(i16) -> i16 {
block0(v0: i16):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit w3, w0
;   orr w5, w3, #32768
;   clz w0, w5
;   ret

function %d(i32) -> i32 {
block0(v0: i32):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit w3, w0
;   clz w0, w3
;   ret

function %d(i64) -> i64 {
block0(v0: i64):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit x3, x0
;   clz x0, x3
;   ret

function %d(i128) -> i128 {
block0(v0: i128):
    v1 = ctz v0
    return v1
}

; block0:
;   rbit x6, x0
;   rbit x8, x1
;   clz x10, x6
;   clz x12, x8
;   lsr x14, x10, #6
;   madd x0, x12, x14, x10
;   movz x1, #0
;   ret

function %d(i128) -> i128 {
block0(v0: i128):
    v1 = popcnt v0
    return v1
}

;   stp fp, lr, [sp, #-16]!
;   mov fp, sp
;   stp d11, d13, [sp, #-16]!
; block0:
;   fmov d6, x0
;   mov v6.d[1], x1
;   cnt v11.16b, v6.16b
;   addv b13, v11.16b
;   umov w0, v13.b[0]
;   movz x1, #0
;   ldp d11, d13, [sp], #16
;   ldp fp, lr, [sp], #16
;   ret

function %d(i64) -> i64 {
block0(v0: i64):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov d3, x0
;   cnt v5.8b, v3.8b
;   addv b7, v5.8b
;   umov w0, v7.b[0]
;   ret

function %d(i32) -> i32 {
block0(v0: i32):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov s3, w0
;   cnt v5.8b, v3.8b
;   addv b7, v5.8b
;   umov w0, v7.b[0]
;   ret

function %d(i16) -> i16 {
block0(v0: i16):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov s3, w0
;   cnt v5.8b, v3.8b
;   addp v7.8b, v5.8b, v5.8b
;   umov w0, v7.b[0]
;   ret

function %d(i8) -> i8 {
block0(v0: i8):
    v1 = popcnt v0
    return v1
}

; block0:
;   fmov s3, w0
;   cnt v5.8b, v3.8b
;   umov w0, v5.b[0]
;   ret

function %bextend_b8() -> b32 {
block0:
    v1 = bconst.b8 true
    v2 = bextend.b32 v1
    return v2
}

; block0:
;   movz x2, #255
;   sxtb w0, w2
;   ret

function %bextend_b1() -> b32 {
block0:
    v1 = bconst.b1 true
    v2 = bextend.b32 v1
    return v2
}

; block0:
;   movz x2, #1
;   sbfx w0, w2, #0, #1
;   ret

function %bnot_i32(i32) -> i32 {
block0(v0: i32):
    v1 = bnot v0
    return v1
}

; block0:
;   orn w0, wzr, w0
;   ret

function %bnot_i64(i64) -> i64 {
block0(v0: i64):
    v1 = bnot v0
    return v1
}

; block0:
;   orn x0, xzr, x0
;   ret

function %bnot_i64_with_shift(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = ishl.i64 v0, v1
    v3 = bnot v2
    return v3
}

; block0:
;   orn x0, xzr, x0, LSL 3
;   ret

function %bnot_i128(i128) -> i128 {
block0(v0: i128):
    v1 = bnot v0
    return v1
}

; block0:
;   orn x0, xzr, x0
;   orn x1, xzr, x1
;   ret

function %bnot_i8x16(i8x16) -> i8x16 {
block0(v0: i8x16):
    v1 = bnot v0
    return v1
}

; block0:
;   mvn v0.16b, v0.16b
;   ret

function %band_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band v0, v1
    return v2
}

; block0:
;   and w0, w0, w1
;   ret

function %band_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = band v0, v1
    return v2
}

; block0:
;   and x0, x0, x1
;   ret

function %band_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = band v0, v1
    return v2
}

; block0:
;   and x0, x0, x2
;   and x1, x1, x3
;   ret

function %band_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = band v0, v1
    return v2
}

; block0:
;   and v0.16b, v0.16b, v1.16b
;   ret

function %band_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = band v0, v1
    return v2
}

; block0:
;   and x0, x0, #3
;   ret

function %band_i64_constant2(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = band v1, v0
    return v2
}

; block0:
;   and x0, x0, #3
;   ret

function %band_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = band v0, v3
    return v4
}

; block0:
;   and x0, x0, x1, LSL 3
;   ret

function %band_i64_constant_shift2(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = band v3, v0
    return v4
}

; block0:
;   and x0, x0, x1, LSL 3
;   ret

function %bor_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr w0, w0, w1
;   ret

function %bor_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr x0, x0, x1
;   ret

function %bor_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr x0, x0, x2
;   orr x1, x1, x3
;   ret

function %bor_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr v0.16b, v0.16b, v1.16b
;   ret

function %bor_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = bor v0, v1
    return v2
}

; block0:
;   orr x0, x0, #3
;   ret

function %bor_i64_constant2(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = bor v1, v0
    return v2
}

; block0:
;   orr x0, x0, #3
;   ret

function %bor_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = bor v0, v3
    return v4
}

; block0:
;   orr x0, x0, x1, LSL 3
;   ret

function %bor_i64_constant_shift2(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = bor v3, v0
    return v4
}

; block0:
;   orr x0, x0, x1, LSL 3
;   ret

function %bxor_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor w0, w0, w1
;   ret

function %bxor_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor x0, x0, x1
;   ret

function %bxor_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor x0, x0, x2
;   eor x1, x1, x3
;   ret

function %bxor_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor v0.16b, v0.16b, v1.16b
;   ret

function %bxor_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = bxor v0, v1
    return v2
}

; block0:
;   eor x0, x0, #3
;   ret

function %bxor_i64_constant2(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 3
    v2 = bxor v1, v0
    return v2
}

; block0:
;   eor x0, x0, #3
;   ret

function %bxor_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = bxor v0, v3
    return v4
}

; block0:
;   eor x0, x0, x1, LSL 3
;   ret

function %bxor_i64_constant_shift2(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 3
    v3 = ishl.i64 v1, v2
    v4 = bxor v3, v0
    return v4
}

; block0:
;   eor x0, x0, x1, LSL 3
;   ret

function %band_not_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic w0, w0, w1
;   ret

function %band_not_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic x0, x0, x1
;   ret

function %band_not_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic x0, x0, x2
;   bic x1, x1, x3
;   ret

function %band_not_i8x16(i8x16, i8x16) -> i8x16 {
block0(v0: i8x16, v1: i8x16):
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic v0.16b, v0.16b, v1.16b
;   ret

function %band_not_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 4
    v2 = band_not v0, v1
    return v2
}

; block0:
;   bic x0, x0, #4
;   ret

function %band_not_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 4
    v3 = ishl.i64 v1, v2
    v4 = band_not v0, v3
    return v4
}

; block0:
;   bic x0, x0, x1, LSL 4
;   ret

function %bor_not_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bor_not v0, v1
    return v2
}

; block0:
;   orn w0, w0, w1
;   ret

function %bor_not_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bor_not v0, v1
    return v2
}

; block0:
;   orn x0, x0, x1
;   ret

function %bor_not_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bor_not v0, v1
    return v2
}

; block0:
;   orn x0, x0, x2
;   orn x1, x1, x3
;   ret

function %bor_not_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 4
    v2 = bor_not v0, v1
    return v2
}

; block0:
;   orn x0, x0, #4
;   ret

function %bor_not_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 4
    v3 = ishl.i64 v1, v2
    v4 = bor_not v0, v3
    return v4
}

; block0:
;   orn x0, x0, x1, LSL 4
;   ret

function %bxor_not_i32(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
    v2 = bxor_not v0, v1
    return v2
}

; block0:
;   eon w0, w0, w1
;   ret

function %bxor_not_i64(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = bxor_not v0, v1
    return v2
}

; block0:
;   eon x0, x0, x1
;   ret

function %bxor_not_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = bxor_not v0, v1
    return v2
}

; block0:
;   eon x0, x0, x2
;   eon x1, x1, x3
;   ret

function %bxor_not_i64_constant(i64) -> i64 {
block0(v0: i64):
    v1 = iconst.i64 4
    v2 = bxor_not v0, v1
    return v2
}

; block0:
;   eon x0, x0, #4
;   ret

function %bxor_not_i64_constant_shift(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
    v2 = iconst.i64 4
    v3 = ishl.i64 v1, v2
    v4 = bxor_not v0, v3
    return v4
}

; block0:
;   eon x0, x0, x1, LSL 4
;   ret

function %ishl_i128_i8(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = ishl.i128 v0, v1
    return v2
}

; block0:
;   lsl x8, x0, x2
;   lsl x10, x1, x2
;   orn w12, wzr, w2
;   lsr x14, x0, #1
;   lsr x0, x14, x12
;   orr x3, x10, x0
;   ands xzr, x2, #64
;   csel x0, xzr, x8, ne
;   csel x1, x8, x3, ne
;   ret

function %ishl_i128_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = ishl.i128 v0, v1
    return v2
}

; block0:
;   lsl x10, x0, x2
;   lsl x12, x1, x2
;   orn w14, wzr, w2
;   lsr x0, x0, #1
;   lsr x3, x0, x14
;   orr x4, x12, x3
;   ands xzr, x2, #64
;   csel x0, xzr, x10, ne
;   csel x1, x10, x4, ne
;   ret

function %ushr_i128_i8(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = ushr.i128 v0, v1
    return v2
}

; block0:
;   lsr x8, x0, x2
;   lsr x10, x1, x2
;   orn w12, wzr, w2
;   lsl x14, x1, #1
;   lsl x0, x14, x12
;   orr x3, x8, x0
;   ands xzr, x2, #64
;   csel x0, x10, x3, ne
;   csel x1, xzr, x10, ne
;   ret

function %ushr_i128_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = ushr.i128 v0, v1
    return v2
}

; block0:
;   lsr x10, x0, x2
;   lsr x12, x1, x2
;   orn w14, wzr, w2
;   lsl x0, x1, #1
;   lsl x3, x0, x14
;   orr x4, x10, x3
;   ands xzr, x2, #64
;   csel x0, x12, x4, ne
;   csel x1, xzr, x12, ne
;   ret

function %sshr_i128_i8(i128, i8) -> i128 {
block0(v0: i128, v1: i8):
    v2 = sshr.i128 v0, v1
    return v2
}

; block0:
;   lsr x8, x0, x2
;   asr x10, x1, x2
;   orn w12, wzr, w2
;   lsl x14, x1, #1
;   lsl x0, x14, x12
;   asr x3, x1, #63
;   orr x4, x8, x0
;   ands xzr, x2, #64
;   csel x0, x10, x4, ne
;   csel x1, x3, x10, ne
;   ret

function %sshr_i128_i128(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
    v2 = sshr.i128 v0, v1
    return v2
}

; block0:
;   lsr x10, x0, x2
;   asr x12, x1, x2
;   orn w14, wzr, w2
;   lsl x0, x1, #1
;   lsl x3, x0, x14
;   asr x4, x1, #63
;   orr x6, x10, x3
;   ands xzr, x2, #64
;   csel x0, x12, x6, ne
;   csel x1, x4, x12, ne
;   ret
