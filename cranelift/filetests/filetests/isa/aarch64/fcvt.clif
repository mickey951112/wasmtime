test compile precise-output
target aarch64

function %f1(i8) -> f32 {
block0(v0: i8):
  v1 = fcvt_from_sint.f32 v0
  return v1
}

; block0:
;   sxtb w3, w0
;   scvtf s0, w3
;   ret

function %f2(i16) -> f32 {
block0(v0: i16):
  v1 = fcvt_from_sint.f32 v0
  return v1
}

; block0:
;   sxth w3, w0
;   scvtf s0, w3
;   ret

function %f3(i32) -> f32 {
block0(v0: i32):
  v1 = fcvt_from_sint.f32 v0
  return v1
}

; block0:
;   scvtf s0, w0
;   ret

function %f4(i64) -> f32 {
block0(v0: i64):
  v1 = fcvt_from_sint.f32 v0
  return v1
}

; block0:
;   scvtf s0, x0
;   ret

function %f5(i8) -> f64 {
block0(v0: i8):
  v1 = fcvt_from_sint.f64 v0
  return v1
}

; block0:
;   sxtb w3, w0
;   scvtf d0, w3
;   ret

function %f6(i16) -> f64 {
block0(v0: i16):
  v1 = fcvt_from_sint.f64 v0
  return v1
}

; block0:
;   sxth w3, w0
;   scvtf d0, w3
;   ret

function %f7(i32) -> f64 {
block0(v0: i32):
  v1 = fcvt_from_sint.f64 v0
  return v1
}

; block0:
;   scvtf d0, w0
;   ret

function %f8(i64) -> f64 {
block0(v0: i64):
  v1 = fcvt_from_sint.f64 v0
  return v1
}

; block0:
;   scvtf d0, x0
;   ret

function %f9(i32x4) -> f64x2 {
block0(v0: i32x4):
  v1 = fcvt_low_from_sint.f64x2 v0
  return v1
}

; block0:
;   sxtl v3.2d, v0.2s
;   scvtf v0.2d, v3.2d
;   ret

function %f10(i8, i16, i32, i64) -> f32 {
block0(v0: i8, v1: i16, v2: i32, v3: i64):
  v4 = fcvt_from_uint.f32 v0
  v5 = fcvt_from_uint.f32 v1
  v6 = fcvt_from_uint.f32 v2
  v7 = fcvt_from_uint.f32 v3
  v8 = fadd.f32 v4, v5
  v9 = fadd.f32 v8, v6
  v10 = fadd.f32 v9, v7
  return v10
}

; block0:
;   uxtb w13, w0
;   ucvtf s23, w13
;   uxth w13, w1
;   ucvtf s24, w13
;   ucvtf s22, w2
;   ucvtf s25, x3
;   fadd s23, s23, s24
;   fadd s22, s23, s22
;   fadd s0, s22, s25
;   ret

function %f11(i32x4) -> f64x2 {
block0(v0: i32x4):
  v1 = uwiden_low v0
  v2 = fcvt_from_uint.f64x2 v1
  return v2
}

; block0:
;   uxtl v4.2d, v0.2s
;   ucvtf v0.2d, v4.2d
;   ret

function %f12(i32x4) -> f32x4 {
block0(v0: i32x4):
  v1 = fcvt_from_uint.f32x4 v0
  return v1
}

; block0:
;   ucvtf v0.4s, v0.4s
;   ret

function %f13(f32) -> i32 {
block0(v0: f32):
  v1 = fcvt_to_uint.i32 v0
  return v1
}

; block0:
;   fcmp s0, s0
;   b.vc 8 ; udf
;   fmov s5, #-1
;   fcmp s0, s5
;   b.gt 8 ; udf
;   movz x10, #20352, LSL #16
;   fmov s18, w10
;   fcmp s0, s18
;   b.lt 8 ; udf
;   fcvtzu w0, s0
;   ret

function %f14(f32) -> i64 {
block0(v0: f32):
  v1 = fcvt_to_uint.i64 v0
  return v1
}

; block0:
;   fcmp s0, s0
;   b.vc 8 ; udf
;   fmov s5, #-1
;   fcmp s0, s5
;   b.gt 8 ; udf
;   movz x10, #24448, LSL #16
;   fmov s18, w10
;   fcmp s0, s18
;   b.lt 8 ; udf
;   fcvtzu x0, s0
;   ret

function %f15(f64) -> i32 {
block0(v0: f64):
  v1 = fcvt_to_uint.i32 v0
  return v1
}

; block0:
;   fcmp d0, d0
;   b.vc 8 ; udf
;   fmov d5, #-1
;   fcmp d0, d5
;   b.gt 8 ; udf
;   movz x10, #16880, LSL #48
;   fmov d18, x10
;   fcmp d0, d18
;   b.lt 8 ; udf
;   fcvtzu w0, d0
;   ret

function %f16(f64) -> i64 {
block0(v0: f64):
  v1 = fcvt_to_uint.i64 v0
  return v1
}

; block0:
;   fcmp d0, d0
;   b.vc 8 ; udf
;   fmov d5, #-1
;   fcmp d0, d5
;   b.gt 8 ; udf
;   movz x10, #17392, LSL #48
;   fmov d18, x10
;   fcmp d0, d18
;   b.lt 8 ; udf
;   fcvtzu x0, d0
;   ret

function %f17(f32) -> i32 {
block0(v0: f32):
  v1 = fcvt_to_uint_sat.i32 v0
  return v1
}

; block0:
;   fcvtzu w0, s0
;   ret

function %f18(f32) -> i64 {
block0(v0: f32):
  v1 = fcvt_to_uint_sat.i64 v0
  return v1
}

; block0:
;   fcvtzu x0, s0
;   ret

function %f19(f64) -> i32 {
block0(v0: f64):
  v1 = fcvt_to_uint_sat.i32 v0
  return v1
}

; block0:
;   fcvtzu w0, d0
;   ret

function %f20(f64) -> i64 {
block0(v0: f64):
  v1 = fcvt_to_uint_sat.i64 v0
  return v1
}

; block0:
;   fcvtzu x0, d0
;   ret

function %f21(f32) -> i32 {
block0(v0: f32):
  v1 = fcvt_to_sint.i32 v0
  return v1
}

; block0:
;   fcmp s0, s0
;   b.vc 8 ; udf
;   movz x6, #52992, LSL #16
;   fmov s6, w6
;   fcmp s0, s6
;   b.ge 8 ; udf
;   movz x12, #20224, LSL #16
;   fmov s20, w12
;   fcmp s0, s20
;   b.lt 8 ; udf
;   fcvtzs w0, s0
;   ret

function %f22(f32) -> i64 {
block0(v0: f32):
  v1 = fcvt_to_sint.i64 v0
  return v1
}

; block0:
;   fcmp s0, s0
;   b.vc 8 ; udf
;   movz x6, #57088, LSL #16
;   fmov s6, w6
;   fcmp s0, s6
;   b.ge 8 ; udf
;   movz x12, #24320, LSL #16
;   fmov s20, w12
;   fcmp s0, s20
;   b.lt 8 ; udf
;   fcvtzs x0, s0
;   ret

function %f23(f64) -> i32 {
block0(v0: f64):
  v1 = fcvt_to_sint.i32 v0
  return v1
}

; block0:
;   fcmp d0, d0
;   b.vc 8 ; udf
;   ldr d5, pc+8 ; b 12 ; data.f64 -2147483649
;   fcmp d0, d5
;   b.gt 8 ; udf
;   movz x10, #16864, LSL #48
;   fmov d18, x10
;   fcmp d0, d18
;   b.lt 8 ; udf
;   fcvtzs w0, d0
;   ret

function %f24(f64) -> i64 {
block0(v0: f64):
  v1 = fcvt_to_sint.i64 v0
  return v1
}

; block0:
;   fcmp d0, d0
;   b.vc 8 ; udf
;   movz x6, #50144, LSL #48
;   fmov d6, x6
;   fcmp d0, d6
;   b.ge 8 ; udf
;   movz x12, #17376, LSL #48
;   fmov d20, x12
;   fcmp d0, d20
;   b.lt 8 ; udf
;   fcvtzs x0, d0
;   ret

function %f25(f32) -> i32 {
block0(v0: f32):
  v1 = fcvt_to_sint_sat.i32 v0
  return v1
}

; block0:
;   fcvtzs w0, s0
;   ret

function %f26(f32) -> i64 {
block0(v0: f32):
  v1 = fcvt_to_sint_sat.i64 v0
  return v1
}

; block0:
;   fcvtzs x0, s0
;   ret

function %f27(f64) -> i32 {
block0(v0: f64):
  v1 = fcvt_to_sint_sat.i32 v0
  return v1
}

; block0:
;   fcvtzs w0, d0
;   ret

function %f28(f64) -> i64 {
block0(v0: f64):
  v1 = fcvt_to_sint_sat.i64 v0
  return v1
}

; block0:
;   fcvtzs x0, d0
;   ret

function %f29(f32x4) -> i32x4 {
block0(v0: f32x4):
  v1 = fcvt_to_uint_sat.i32x4 v0
  return v1
}

; block0:
;   fcvtzu v0.4s, v0.4s
;   ret

function %f30(f32x4) -> i32x4 {
block0(v0: f32x4):
  v1 = fcvt_to_sint_sat.i32x4 v0
  return v1
}

; block0:
;   fcvtzs v0.4s, v0.4s
;   ret

function %f31(f32) -> i8 {
block0(v0: f32):
  v1 = fcvt_to_uint_sat.i8 v0
  return v1
}

; block0:
;   fcvtzu w3, s0
;   movz w5, #255
;   subs wzr, w3, w5
;   csel x0, x5, x3, hi
;   ret

function %f32(f32) -> i8 {
block0(v0: f32):
  v1 = fcvt_to_sint_sat.i8 v0
  return v1
}

; block0:
;   fcvtzs w3, s0
;   movz w5, #127
;   movn x7, #127
;   subs wzr, w3, w5
;   csel x10, x5, x3, gt
;   subs wzr, w10, w7
;   csel x0, x7, x10, lt
;   ret

function %f33(f32) -> i16 {
block0(v0: f32):
  v1 = fcvt_to_uint_sat.i16 v0
  return v1
}

; block0:
;   fcvtzu w3, s0
;   movz w5, #65535
;   subs wzr, w3, w5
;   csel x0, x5, x3, hi
;   ret

function %f34(f32) -> i16 {
block0(v0: f32):
  v1 = fcvt_to_sint_sat.i16 v0
  return v1
}

; block0:
;   fcvtzs w3, s0
;   movz w5, #32767
;   movn x7, #32767
;   subs wzr, w3, w5
;   csel x10, x5, x3, gt
;   subs wzr, w10, w7
;   csel x0, x7, x10, lt
;   ret

function %f35(f64) -> i8 {
block0(v0: f64):
  v1 = fcvt_to_uint_sat.i8 v0
  return v1
}

; block0:
;   fcvtzu w3, d0
;   movz w5, #255
;   subs wzr, w3, w5
;   csel x0, x5, x3, hi
;   ret

function %f36(f64) -> i8 {
block0(v0: f64):
  v1 = fcvt_to_sint_sat.i8 v0
  return v1
}

; block0:
;   fcvtzs w3, d0
;   movz w5, #127
;   movn x7, #127
;   subs wzr, w3, w5
;   csel x10, x5, x3, gt
;   subs wzr, w10, w7
;   csel x0, x7, x10, lt
;   ret

function %f37(f64) -> i16 {
block0(v0: f64):
  v1 = fcvt_to_uint_sat.i16 v0
  return v1
}

; block0:
;   fcvtzu w3, d0
;   movz w5, #65535
;   subs wzr, w3, w5
;   csel x0, x5, x3, hi
;   ret

function %f38(f64) -> i16 {
block0(v0: f64):
  v1 = fcvt_to_sint_sat.i16 v0
  return v1
}

; block0:
;   fcvtzs w3, d0
;   movz w5, #32767
;   movn x7, #32767
;   subs wzr, w3, w5
;   csel x10, x5, x3, gt
;   subs wzr, w10, w7
;   csel x0, x7, x10, lt
;   ret

