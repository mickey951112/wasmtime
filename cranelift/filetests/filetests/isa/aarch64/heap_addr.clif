test compile precise-output
set unwind_info=false
set enable_heap_access_spectre_mitigation=true
target aarch64

function %dynamic_heap_check(i64 vmctx, i32) -> i64 {
    gv0 = vmctx
    gv1 = load.i64 notrap aligned gv0
    heap0 = dynamic gv0, bound gv1, offset_guard 0x1000, index_type i32

block0(v0: i64, v1: i32):
    v2 = heap_addr.i64 heap0, v1, 0, 0
    return v2
}

; block0:
;   mov w8, w1
;   ldr x9, [x0]
;   mov x9, x9
;   add x10, x0, x1, UXTW
;   movz x7, #0
;   subs xzr, x8, x9
;   csel x0, x7, x10, hi
;   csdb
;   ret

function %static_heap_check(i64 vmctx, i32) -> i64 {
    gv0 = vmctx
    heap0 = static gv0, bound 0x1_0000, offset_guard 0x1000, index_type i32

block0(v0: i64, v1: i32):
    v2 = heap_addr.i64 heap0, v1, 0, 0
    return v2
}

; block0:
;   mov w6, w1
;   add x7, x0, x1, UXTW
;   movz x5, #0
;   subs xzr, x6, #65536
;   csel x0, x5, x7, hi
;   csdb
;   ret

function %dynamic_heap_check_with_offset(i64 vmctx, i32) -> i64 {
    gv0 = vmctx
    gv1 = load.i64 notrap aligned gv0
    heap0 = dynamic gv0, bound gv1, offset_guard 0x1000, index_type i32

block0(v0: i64, v1: i32):
    v2 = heap_addr.i64 heap0, v1, 16, 8
    return v2
}

; block0:
;   mov w10, w1
;   movz x9, #24
;   adds x11, x10, x9
;   b.lo 8 ; udf
;   ldr x12, [x0]
;   add x13, x0, x1, UXTW
;   add x13, x13, #16
;   movz x10, #0
;   subs xzr, x11, x12
;   csel x0, x10, x13, hi
;   csdb
;   ret

function %static_heap_check_with_offset(i64 vmctx, i32) -> i64 {
    gv0 = vmctx
    heap0 = static gv0, bound 0x1_0000, offset_guard 0x1000, index_type i32

block0(v0: i64, v1: i32):
    v2 = heap_addr.i64 heap0, v1, 16, 8
    return v2
}

; block0:
;   mov w8, w1
;   add x9, x0, x1, UXTW
;   add x9, x9, #16
;   movz w6, #65512
;   movz x10, #0
;   subs xzr, x8, x6
;   csel x0, x10, x9, hi
;   csdb
;   ret

