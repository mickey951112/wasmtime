test compile
target aarch64

function %snarrow_i16x8(i16) -> i8x16 {
  gv0 = dyn_scale_target_const.i16x8
  gv1 = dyn_scale_target_const.i8x16
  dt0 = i16x8*gv0
  dt1 = i8x16*gv0

block0(v0: i16):
  v1 = splat.dt0 v0
  v2 = snarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.8h, w0
; nextln: sqxtn v0.8b, v2.8h
; nextln: sqxtn2 v0.16b, v2.8h
; nextln: ret

function %snarrow_i32x4(i32) -> i16x8 {
  gv0 = dyn_scale_target_const.i32x4
  gv1 = dyn_scale_target_const.i16x8
  dt0 = i32x4*gv0
  dt1 = i16x8*gv0

block0(v0: i32):
  v1 = splat.dt0 v0
  v2 = snarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.4s, w0
; nextln: sqxtn v0.4h, v2.4s
; nextln: sqxtn2 v0.8h, v2.4s
; nextln: ret

function %snarrow_i64x2(i64) -> i32x4 {
  gv0 = dyn_scale_target_const.i64x2
  gv1 = dyn_scale_target_const.i32x4
  dt0 = i64x2*gv0
  dt1 = i32x4*gv0

block0(v0: i64):
  v1 = splat.dt0 v0
  v2 = snarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.2d, x0
; nextln: sqxtn v0.2s, v2.2d
; nextln: sqxtn2 v0.4s, v2.2d
; nextln: ret

function %unarrow_i16x8(i16) -> i8x16 {
  gv0 = dyn_scale_target_const.i16x8
  gv1 = dyn_scale_target_const.i8x16
  dt0 = i16x8*gv0
  dt1 = i8x16*gv0

block0(v0: i16):
  v1 = splat.dt0 v0
  v2 = unarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.8h, w0
; nextln: sqxtun v0.8b, v2.8h
; nextln: sqxtun2 v0.16b, v2.8h
; nextln: ret

function %unarrow_i32x4(i32) -> i16x8 {
  gv0 = dyn_scale_target_const.i32x4
  gv1 = dyn_scale_target_const.i16x8
  dt0 = i32x4*gv0
  dt1 = i16x8*gv0

block0(v0: i32):
  v1 = splat.dt0 v0
  v2 = unarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.4s, w0
; nextln: sqxtun v0.4h, v2.4s
; nextln: sqxtun2 v0.8h, v2.4s
; nextln: ret

function %unarrow_i64x2(i64) -> i32x4 {
  gv0 = dyn_scale_target_const.i64x2
  gv1 = dyn_scale_target_const.i32x4
  dt0 = i64x2*gv0
  dt1 = i32x4*gv0

block0(v0: i64):
  v1 = splat.dt0 v0
  v2 = unarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.2d, x0
; nextln: sqxtun v0.2s, v2.2d
; nextln: sqxtun2 v0.4s, v2.2d
; nextln: ret

function %uunarrow_i16x8(i16) -> i8x16 {
  gv0 = dyn_scale_target_const.i16x8
  gv1 = dyn_scale_target_const.i8x16
  dt0 = i16x8*gv0
  dt1 = i8x16*gv0

block0(v0: i16):
  v1 = splat.dt0 v0
  v2 = uunarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.8h, w0
; nextln: uqxtn v0.8b, v2.8h
; nextln: uqxtn2 v0.16b, v2.8h
; nextln: ret

function %uunarrow_i32x4(i32) -> i16x8 {
  gv0 = dyn_scale_target_const.i32x4
  gv1 = dyn_scale_target_const.i16x8
  dt0 = i32x4*gv0
  dt1 = i16x8*gv0

block0(v0: i32):
  v1 = splat.dt0 v0
  v2 = uunarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.4s, w0
; nextln: uqxtn v0.4h, v2.4s
; nextln: uqxtn2 v0.8h, v2.4s
; nextln: ret

function %uunarrow_i64x2(i64) -> i32x4 {
  gv0 = dyn_scale_target_const.i64x2
  gv1 = dyn_scale_target_const.i32x4
  dt0 = i64x2*gv0
  dt1 = i32x4*gv0

block0(v0: i64):
  v1 = splat.dt0 v0
  v2 = uunarrow.dt0 v1, v1
  v3 = extract_vector v2, 0
  return v3
}

; check: dup v2.2d, x0
; nextln: uqxtn v0.2s, v2.2d
; nextln: uqxtn2 v0.4s, v2.2d
; nextln: ret
