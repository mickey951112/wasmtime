test compile precise-output
set unwind_info=false
target riscv64

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; ROR, variable
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

function %i128_rotr(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
  v2 = rotr.i128 v0, v1
  return v2
}

; VCode:
; block0:
;   andi a3,a2,63
;   li a4,64
;   sub a6,a4,a3
;   srl t3,a0,a3
;   sll t0,a1,a6
;   mv t1,a1
;   select_reg t2,zero,t0##condition=(a3 eq zero)
;   or a1,t3,t2
;   srl a4,t1,a3
;   sll a5,a0,a6
;   select_reg a7,zero,a5##condition=(a3 eq zero)
;   or t4,a4,a7
;   li t1,64
;   andi a2,a2,127
;   select_reg a0,t4,a1##condition=(a2 uge t1)
;   select_reg a1,a1,t4##condition=(a2 uge t1)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a3, a2, 0x3f
;   addi a4, zero, 0x40
;   sub a6, a4, a3
;   srl t3, a0, a3
;   sll t0, a1, a6
;   ori t1, a1, 0
;   beqz a3, 0xc
;   ori t2, t0, 0
;   j 8
;   ori t2, zero, 0
;   or a1, t3, t2
;   srl a4, t1, a3
;   sll a5, a0, a6
;   beqz a3, 0xc
;   ori a7, a5, 0
;   j 8
;   ori a7, zero, 0
;   or t4, a4, a7
;   addi t1, zero, 0x40
;   andi a2, a2, 0x7f
;   bgeu a2, t1, 0xc
;   ori a0, a1, 0
;   j 8
;   ori a0, t4, 0
;   bgeu a2, t1, 8
;   ori a1, t4, 0
;   ret

function %f0(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = rotr.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   mv a7,a0
;   andi a0,a1,63
;   li a2,64
;   sub a4,a2,a0
;   mv t0,a7
;   srl a6,t0,a0
;   sll t3,t0,a4
;   select_reg t0,zero,t3##condition=(a0 eq zero)
;   or a0,a6,t0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ori a7, a0, 0
;   andi a0, a1, 0x3f
;   addi a2, zero, 0x40
;   sub a4, a2, a0
;   ori t0, a7, 0
;   srl a6, t0, a0
;   sll t3, t0, a4
;   beqz a0, 0xc
;   ori t0, t3, 0
;   j 8
;   ori t0, zero, 0
;   or a0, a6, t0
;   ret

function %f1(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = rotr.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,32
;   srli a2,a0,32
;   andi a4,a1,31
;   li a6,32
;   sub t3,a6,a4
;   srl t0,a2,a4
;   sll t2,a2,t3
;   select_reg a1,zero,t2##condition=(a4 eq zero)
;   or a0,t0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x20
;   srli a2, a0, 0x20
;   andi a4, a1, 0x1f
;   addi a6, zero, 0x20
;   sub t3, a6, a4
;   srl t0, a2, a4
;   sll t2, a2, t3
;   beqz a4, 0xc
;   ori a1, t2, 0
;   j 8
;   ori a1, zero, 0
;   or a0, t0, a1
;   ret

function %f2(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
  v2 = rotr.i16 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srli a2,a0,48
;   andi a4,a1,15
;   li a6,16
;   sub t3,a6,a4
;   srl t0,a2,a4
;   sll t2,a2,t3
;   select_reg a1,zero,t2##condition=(a4 eq zero)
;   or a0,t0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srli a2, a0, 0x30
;   andi a4, a1, 0xf
;   addi a6, zero, 0x10
;   sub t3, a6, a4
;   srl t0, a2, a4
;   sll t2, a2, t3
;   beqz a4, 0xc
;   ori a1, t2, 0
;   j 8
;   ori a1, zero, 0
;   or a0, t0, a1
;   ret

function %f3(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
  v2 = rotr.i8 v0, v1
  return v2
}

; VCode:
; block0:
;   andi a0,a0,255
;   andi a2,a1,7
;   li a4,8
;   sub a6,a4,a2
;   srl t3,a0,a2
;   sll t0,a0,a6
;   select_reg t2,zero,t0##condition=(a2 eq zero)
;   or a0,t3,t2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a0, a0, 0xff
;   andi a2, a1, 7
;   addi a4, zero, 8
;   sub a6, a4, a2
;   srl t3, a0, a2
;   sll t0, a0, a6
;   beqz a2, 0xc
;   ori t2, t0, 0
;   j 8
;   ori t2, zero, 0
;   or a0, t3, t2
;   ret

function %i128_rotl(i128, i128) -> i128 {
block0(v0: i128, v1: i128):
  v2 = rotl.i128 v0, v1
  return v2
}

; VCode:
; block0:
;   andi a3,a2,63
;   li a4,64
;   sub a6,a4,a3
;   sll t3,a0,a3
;   srl t0,a1,a6
;   mv t1,a1
;   select_reg t2,zero,t0##condition=(a3 eq zero)
;   or a1,t3,t2
;   sll a4,t1,a3
;   srl a5,a0,a6
;   select_reg a7,zero,a5##condition=(a3 eq zero)
;   or t4,a4,a7
;   li t1,64
;   andi a2,a2,127
;   select_reg a0,t4,a1##condition=(a2 uge t1)
;   select_reg a1,a1,t4##condition=(a2 uge t1)
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a3, a2, 0x3f
;   addi a4, zero, 0x40
;   sub a6, a4, a3
;   sll t3, a0, a3
;   srl t0, a1, a6
;   ori t1, a1, 0
;   beqz a3, 0xc
;   ori t2, t0, 0
;   j 8
;   ori t2, zero, 0
;   or a1, t3, t2
;   sll a4, t1, a3
;   srl a5, a0, a6
;   beqz a3, 0xc
;   ori a7, a5, 0
;   j 8
;   ori a7, zero, 0
;   or t4, a4, a7
;   addi t1, zero, 0x40
;   andi a2, a2, 0x7f
;   bgeu a2, t1, 0xc
;   ori a0, a1, 0
;   j 8
;   ori a0, t4, 0
;   bgeu a2, t1, 8
;   ori a1, t4, 0
;   ret

function %f4(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = rotl.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   mv a7,a0
;   andi a0,a1,63
;   li a2,64
;   sub a4,a2,a0
;   mv t0,a7
;   sll a6,t0,a0
;   srl t3,t0,a4
;   select_reg t0,zero,t3##condition=(a0 eq zero)
;   or a0,a6,t0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   ori a7, a0, 0
;   andi a0, a1, 0x3f
;   addi a2, zero, 0x40
;   sub a4, a2, a0
;   ori t0, a7, 0
;   sll a6, t0, a0
;   srl t3, t0, a4
;   beqz a0, 0xc
;   ori t0, t3, 0
;   j 8
;   ori t0, zero, 0
;   or a0, a6, t0
;   ret

function %f5(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = rotl.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,32
;   srli a2,a0,32
;   andi a4,a1,31
;   li a6,32
;   sub t3,a6,a4
;   sll t0,a2,a4
;   srl t2,a2,t3
;   select_reg a1,zero,t2##condition=(a4 eq zero)
;   or a0,t0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x20
;   srli a2, a0, 0x20
;   andi a4, a1, 0x1f
;   addi a6, zero, 0x20
;   sub t3, a6, a4
;   sll t0, a2, a4
;   srl t2, a2, t3
;   beqz a4, 0xc
;   ori a1, t2, 0
;   j 8
;   ori a1, zero, 0
;   or a0, t0, a1
;   ret

function %f6(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
  v2 = rotl.i16 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srli a2,a0,48
;   andi a4,a1,15
;   li a6,16
;   sub t3,a6,a4
;   sll t0,a2,a4
;   srl t2,a2,t3
;   select_reg a1,zero,t2##condition=(a4 eq zero)
;   or a0,t0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srli a2, a0, 0x30
;   andi a4, a1, 0xf
;   addi a6, zero, 0x10
;   sub t3, a6, a4
;   sll t0, a2, a4
;   srl t2, a2, t3
;   beqz a4, 0xc
;   ori a1, t2, 0
;   j 8
;   ori a1, zero, 0
;   or a0, t0, a1
;   ret

function %f7(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
  v2 = rotl.i8 v0, v1
  return v2
}

; VCode:
; block0:
;   andi a0,a0,255
;   andi a2,a1,7
;   li a4,8
;   sub a6,a4,a2
;   sll t3,a0,a2
;   srl t0,a0,a6
;   select_reg t2,zero,t0##condition=(a2 eq zero)
;   or a0,t3,t2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a0, a0, 0xff
;   andi a2, a1, 7
;   addi a4, zero, 8
;   sub a6, a4, a2
;   sll t3, a0, a2
;   srl t0, a0, a6
;   beqz a2, 0xc
;   ori t2, t0, 0
;   j 8
;   ori t2, zero, 0
;   or a0, t3, t2
;   ret

function %f8(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = ushr.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   srl a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   srl a0, a0, a1
;   ret

function %f9(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = ushr.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   srlw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   srlw a0, a0, a1
;   ret

function %f10(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
  v2 = ushr.i16 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srli a2,a0,48
;   andi a4,a1,15
;   srlw a0,a2,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srli a2, a0, 0x30
;   andi a4, a1, 0xf
;   srlw a0, a2, a4
;   ret

function %f11(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
  v2 = ushr.i8 v0, v1
  return v2
}

; VCode:
; block0:
;   andi a0,a0,255
;   andi a2,a1,7
;   srlw a0,a0,a2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a0, a0, 0xff
;   andi a2, a1, 7
;   srlw a0, a0, a2
;   ret

function %f12(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = ishl.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   sll a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sll a0, a0, a1
;   ret

function %f13(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = ishl.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   sllw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sllw a0, a0, a1
;   ret

function %f14(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
  v2 = ishl.i16 v0, v1
  return v2
}

; VCode:
; block0:
;   andi a1,a1,15
;   sllw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a1, a1, 0xf
;   sllw a0, a0, a1
;   ret

function %f15(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
  v2 = ishl.i8 v0, v1
  return v2
}

; VCode:
; block0:
;   andi a1,a1,7
;   sllw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   andi a1, a1, 7
;   sllw a0, a0, a1
;   ret

function %f16(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = sshr.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   sra a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sra a0, a0, a1
;   ret

function %f17(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = sshr.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   sraw a0,a0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   sraw a0, a0, a1
;   ret

function %f18(i16, i16) -> i16 {
block0(v0: i16, v1: i16):
  v2 = sshr.i16 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,48
;   srai a2,a0,48
;   andi a4,a1,15
;   sra a0,a2,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x30
;   srai a2, a0, 0x30
;   andi a4, a1, 0xf
;   sra a0, a2, a4
;   ret

function %f19(i8, i8) -> i8 {
block0(v0: i8, v1: i8):
  v2 = sshr.i8 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,56
;   srai a2,a0,56
;   andi a4,a1,7
;   sra a0,a2,a4
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x38
;   srai a2, a0, 0x38
;   andi a4, a1, 7
;   sra a0, a2, a4
;   ret

function %f20(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i32 17
  v2 = rotr.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   li a6,17
;   andi a1,a6,63
;   li a2,64
;   sub a4,a2,a1
;   srl a6,a0,a1
;   sll t3,a0,a4
;   select_reg t0,zero,t3##condition=(a1 eq zero)
;   or a0,a6,t0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a6, zero, 0x11
;   andi a1, a6, 0x3f
;   addi a2, zero, 0x40
;   sub a4, a2, a1
;   srl a6, a0, a1
;   sll t3, a0, a4
;   beqz a1, 0xc
;   ori t0, t3, 0
;   j 8
;   ori t0, zero, 0
;   or a0, a6, t0
;   ret

function %f21(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i32 17
  v2 = rotl.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   li a6,17
;   andi a1,a6,63
;   li a2,64
;   sub a4,a2,a1
;   sll a6,a0,a1
;   srl t3,a0,a4
;   select_reg t0,zero,t3##condition=(a1 eq zero)
;   or a0,a6,t0
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a6, zero, 0x11
;   andi a1, a6, 0x3f
;   addi a2, zero, 0x40
;   sub a4, a2, a1
;   sll a6, a0, a1
;   srl t3, a0, a4
;   beqz a1, 0xc
;   ori t0, t3, 0
;   j 8
;   ori t0, zero, 0
;   or a0, a6, t0
;   ret

function %f22(i32) -> i32 {
block0(v0: i32):
  v1 = iconst.i32 17
  v2 = rotl.i32 v0, v1
  return v2
}

; VCode:
; block0:
;   li t3,17
;   slli a0,a0,32
;   srli a2,a0,32
;   andi a4,t3,31
;   li a6,32
;   sub t3,a6,a4
;   sll t0,a2,a4
;   srl t2,a2,t3
;   select_reg a1,zero,t2##condition=(a4 eq zero)
;   or a0,t0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi t3, zero, 0x11
;   slli a0, a0, 0x20
;   srli a2, a0, 0x20
;   andi a4, t3, 0x1f
;   addi a6, zero, 0x20
;   sub t3, a6, a4
;   sll t0, a2, a4
;   srl t2, a2, t3
;   beqz a4, 0xc
;   ori a1, t2, 0
;   j 8
;   ori a1, zero, 0
;   or a0, t0, a1
;   ret

function %f23(i16) -> i16 {
block0(v0: i16):
  v1 = iconst.i32 10
  v2 = rotl.i16 v0, v1
  return v2
}

; VCode:
; block0:
;   li t3,10
;   slli a0,a0,48
;   srli a2,a0,48
;   andi a4,t3,15
;   li a6,16
;   sub t3,a6,a4
;   sll t0,a2,a4
;   srl t2,a2,t3
;   select_reg a1,zero,t2##condition=(a4 eq zero)
;   or a0,t0,a1
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi t3, zero, 0xa
;   slli a0, a0, 0x30
;   srli a2, a0, 0x30
;   andi a4, t3, 0xf
;   addi a6, zero, 0x10
;   sub t3, a6, a4
;   sll t0, a2, a4
;   srl t2, a2, t3
;   beqz a4, 0xc
;   ori a1, t2, 0
;   j 8
;   ori a1, zero, 0
;   or a0, t0, a1
;   ret

function %f24(i8) -> i8 {
block0(v0: i8):
  v1 = iconst.i32 3
  v2 = rotl.i8 v0, v1
  return v2
}

; VCode:
; block0:
;   li a7,3
;   andi a0,a0,255
;   andi a2,a7,7
;   li a4,8
;   sub a6,a4,a2
;   sll t3,a0,a2
;   srl t0,a0,a6
;   select_reg t2,zero,t0##condition=(a2 eq zero)
;   or a0,t3,t2
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   addi a7, zero, 3
;   andi a0, a0, 0xff
;   andi a2, a7, 7
;   addi a4, zero, 8
;   sub a6, a4, a2
;   sll t3, a0, a2
;   srl t0, a0, a6
;   beqz a2, 0xc
;   ori t2, t0, 0
;   j 8
;   ori t2, zero, 0
;   or a0, t3, t2
;   ret

function %f25(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i32 17
  v2 = ushr.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   srli a0,a0,17
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   srli a0, a0, 0x11
;   ret

function %f26(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i32 17
  v2 = sshr.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   srai a0,a0,17
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   srai a0, a0, 0x11
;   ret

function %f27(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i32 17
  v2 = ishl.i64 v0, v1
  return v2
}

; VCode:
; block0:
;   slli a0,a0,17
;   ret
;
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x11
;   ret

