test compile precise-output
set unwind_info=false
target riscv64

function %f5(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v0, v2
  v4 = load.i32 v3
  return v4
}

; VCode:
; block0:
;   sext.w a2,a1
;   add a2,a0,a2
;   lw a0,0(a2)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a1, 0x20
;   srai a2, a2, 0x20
;   add a2, a0, a2
;   lw a0, 0(a2)
;   ret

function %f6(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v2, v0
  v4 = load.i32 v3
  return v4
}

; VCode:
; block0:
;   sext.w a2,a1
;   add a2,a2,a0
;   lw a0,0(a2)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a1, 0x20
;   srai a2, a2, 0x20
;   add a2, a2, a0
;   lw a0, 0(a2)
;   ret

function %f7(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = uextend.i64 v0
  v3 = uextend.i64 v1
  v4 = iadd.i64 v2, v3
  v5 = load.i32 v4
  return v5
}

; VCode:
; block0:
;   uext.w a3,a0
;   uext.w a4,a1
;   add a3,a3,a4
;   lw a0,0(a3)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli a3, a0, 0x20
;   srli a3, a3, 0x20
;   slli a4, a1, 0x20
;   srli a4, a4, 0x20
;   add a3, a3, a4
;   lw a0, 0(a3)
;   ret

function %f8(i64, i32) -> i32 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iconst.i64 32
  v4 = iadd.i64 v2, v3
  v5 = iadd.i64 v4, v0
  v6 = iadd.i64 v5, v5
  v7 = load.i32 v6+4
  return v7
}

; VCode:
; block0:
;   sext.w a4,a1
;   addi a4,a4,32
;   add a4,a4,a0
;   add a4,a4,a4
;   lw a0,4(a4)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli a4, a1, 0x20
;   srai a4, a4, 0x20
;   addi a4, a4, 0x20
;   add a4, a4, a0
;   add a4, a4, a4
;   lw a0, 4(a4)
;   ret

function %f9(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 48
  v4 = iadd.i64 v0, v1
  v5 = iadd.i64 v4, v2
  v6 = iadd.i64 v5, v3
  v7 = load.i32 v6
  return v7
}

; VCode:
; block0:
;   add a4,a0,a1
;   add a4,a4,a2
;   addi a4,a4,48
;   lw a0,0(a4)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   add a4, a0, a1
;   add a4, a4, a2
;   addi a4, a4, 0x30
;   lw a0, 0(a4)
;   ret

function %f10(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i64 4100
  v4 = iadd.i64 v0, v1
  v5 = iadd.i64 v4, v2
  v6 = iadd.i64 v5, v3
  v7 = load.i32 v6
  return v7
}

; VCode:
; block0:
;   add a6,a0,a1
;   add a6,a6,a2
;   lui a5,1
;   addi a5,a5,4
;   add t3,a6,a5
;   lw a0,0(t3)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   add a6, a0, a1
;   add a6, a6, a2
;   lui a5, 1
;   addi a5, a5, 4
;   add t3, a6, a5
;   lw a0, 0(t3)
;   ret

function %f10() -> i32 {
block0:
  v1 = iconst.i64 1234
  v2 = load.i32 v1
  return v2
}

; VCode:
; block0:
;   li t0,1234
;   lw a0,0(t0)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   addi t0, zero, 0x4d2
;   lw a0, 0(t0)
;   ret

function %f11(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 8388608 ;; Imm12: 0x800 << 12
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode:
; block0:
;   lui a1,2048
;   add a2,a0,a1
;   lw a0,0(a2)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   lui a1, 0x800
;   add a2, a0, a1
;   lw a0, 0(a2)
;   ret

function %f12(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 -4
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode:
; block0:
;   addi a0,a0,-4
;   lw a0,0(a0)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   addi a0, a0, -4
;   lw a0, 0(a0)
;   ret

function %f13(i64) -> i32 {
block0(v0: i64):
  v1 = iconst.i64 1000000000
  v2 = iadd.i64 v0, v1
  v3 = load.i32 v2
  return v3
}

; VCode:
; block0:
;   lui a1,244141
;   addi a1,a1,2560
;   add a4,a0,a1
;   lw a0,0(a4)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   lui a1, 0x3b9ad
;   addi a1, a1, -0x600
;   add a4, a0, a1
;   lw a0, 0(a4)
;   ret

function %f14(i32) -> i32 {
block0(v0: i32):
  v1 = sextend.i64 v0
  v2 = load.i32 v1
  return v2
}

; VCode:
; block0:
;   sext.w a0,a0
;   lw a0,0(a0)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli a0, a0, 0x20
;   srai a0, a0, 0x20
;   lw a0, 0(a0)
;   ret

function %f15(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = sextend.i64 v0
  v3 = sextend.i64 v1
  v4 = iadd.i64 v2, v3
  v5 = load.i32 v4
  return v5
}

; VCode:
; block0:
;   sext.w a3,a0
;   sext.w a4,a1
;   add a3,a3,a4
;   lw a0,0(a3)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli a3, a0, 0x20
;   srai a3, a3, 0x20
;   slli a4, a1, 0x20
;   srai a4, a4, 0x20
;   add a3, a3, a4
;   lw a0, 0(a3)
;   ret

function %f18(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 -4098
  v6 = uextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode:
; block0:
;   lui a3,1048575
;   addi a3,a3,4094
;   uext.w a6,a3
;   lh a0,0(a6)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   lui a3, 0xfffff
;   addi a3, a3, -2
;   slli a6, a3, 0x20
;   srli a6, a6, 0x20
;   lh a0, 0(a6)
;   ret

function %f19(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 4098
  v6 = uextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode:
; block0:
;   lui a3,1
;   addi a3,a3,2
;   uext.w a6,a3
;   lh a0,0(a6)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   lui a3, 1
;   addi a3, a3, 2
;   slli a6, a3, 0x20
;   srli a6, a6, 0x20
;   lh a0, 0(a6)
;   ret

function %f20(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 -4098
  v6 = sextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode:
; block0:
;   lui a3,1048575
;   addi a3,a3,4094
;   sext.w a6,a3
;   lh a0,0(a6)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   lui a3, 0xfffff
;   addi a3, a3, -2
;   slli a6, a3, 0x20
;   srai a6, a6, 0x20
;   lh a0, 0(a6)
;   ret

function %f21(i64, i64, i64) -> i32 {
block0(v0: i64, v1: i64, v2: i64):
  v3 = iconst.i32 4098
  v6 = sextend.i64 v3
  v5 = sload16.i32 v6+0
  return v5
}

; VCode:
; block0:
;   lui a3,1
;   addi a3,a3,2
;   sext.w a6,a3
;   lh a0,0(a6)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   lui a3, 1
;   addi a3, a3, 2
;   slli a6, a3, 0x20
;   srai a6, a6, 0x20
;   lh a0, 0(a6)
;   ret

function %i128(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0
  store.i128 v1, v0
  return v1
}

; VCode:
; block0:
;   ld t2,0(a0)
;   mv a2,t2
;   ld a1,8(a0)
;   mv a3,a2
;   sd a3,0(a0)
;   sd a1,8(a0)
;   mv a0,a2
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   ld t2, 0(a0)
;   ori a2, t2, 0
;   ld a1, 8(a0)
;   ori a3, a2, 0
;   sd a3, 0(a0)
;   sd a1, 8(a0)
;   ori a0, a2, 0
;   ret

function %i128_imm_offset(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0+16
  store.i128 v1, v0+16
  return v1
}

; VCode:
; block0:
;   ld t2,16(a0)
;   mv a2,t2
;   ld a1,24(a0)
;   mv a3,a2
;   sd a3,16(a0)
;   sd a1,24(a0)
;   mv a0,a2
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   ld t2, 0x10(a0)
;   ori a2, t2, 0
;   ld a1, 0x18(a0)
;   ori a3, a2, 0
;   sd a3, 0x10(a0)
;   sd a1, 0x18(a0)
;   ori a0, a2, 0
;   ret

function %i128_imm_offset_large(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0+504
  store.i128 v1, v0+504
  return v1
}

; VCode:
; block0:
;   ld t2,504(a0)
;   mv a2,t2
;   ld a1,512(a0)
;   mv a3,a2
;   sd a3,504(a0)
;   sd a1,512(a0)
;   mv a0,a2
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   ld t2, 0x1f8(a0)
;   ori a2, t2, 0
;   ld a1, 0x200(a0)
;   ori a3, a2, 0
;   sd a3, 0x1f8(a0)
;   sd a1, 0x200(a0)
;   ori a0, a2, 0
;   ret

function %i128_imm_offset_negative_large(i64) -> i128 {
block0(v0: i64):
  v1 = load.i128 v0-512
  store.i128 v1, v0-512
  return v1
}

; VCode:
; block0:
;   ld t2,-512(a0)
;   mv a2,t2
;   ld a1,-504(a0)
;   mv a3,a2
;   sd a3,-512(a0)
;   sd a1,-504(a0)
;   mv a0,a2
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   ld t2, -0x200(a0)
;   ori a2, t2, 0
;   ld a1, -0x1f8(a0)
;   ori a3, a2, 0
;   sd a3, -0x200(a0)
;   sd a1, -0x1f8(a0)
;   ori a0, a2, 0
;   ret

function %i128_add_offset(i64) -> i128 {
block0(v0: i64):
  v1 = iadd_imm v0, 32
  v2 = load.i128 v1
  store.i128 v2, v1
  return v2
}

; VCode:
; block0:
;   addi a2,a0,32
;   ld a0,0(a2)
;   ld a1,8(a2)
;   sd a0,0(a2)
;   sd a1,8(a2)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   addi a2, a0, 0x20
;   ld a0, 0(a2)
;   ld a1, 8(a2)
;   sd a0, 0(a2)
;   sd a1, 8(a2)
;   ret

function %i128_32bit_sextend_simple(i32) -> i128 {
block0(v0: i32):
  v1 = sextend.i64 v0
  v2 = load.i128 v1
  store.i128 v2, v1
  return v2
}

; VCode:
; block0:
;   sext.w a2,a0
;   ld a0,0(a2)
;   ld a1,8(a2)
;   sd a0,0(a2)
;   sd a1,8(a2)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli a2, a0, 0x20
;   srai a2, a2, 0x20
;   ld a0, 0(a2)
;   ld a1, 8(a2)
;   sd a0, 0(a2)
;   sd a1, 8(a2)
;   ret

function %i128_32bit_sextend(i64, i32) -> i128 {
block0(v0: i64, v1: i32):
  v2 = sextend.i64 v1
  v3 = iadd.i64 v0, v2
  v4 = iadd_imm.i64 v3, 24
  v5 = load.i128 v4
  store.i128 v5, v4
  return v5
}

; VCode:
; block0:
;   sext.w a4,a1
;   add a4,a0,a4
;   addi a4,a4,24
;   ld a0,0(a4)
;   ld a1,8(a4)
;   sd a0,0(a4)
;   sd a1,8(a4)
;   ret
; 
; Disassembled:
; block0: ; offset 0x0
;   slli a4, a1, 0x20
;   srai a4, a4, 0x20
;   add a4, a0, a4
;   addi a4, a4, 0x18
;   ld a0, 0(a4)
;   ld a1, 8(a4)
;   sd a0, 0(a4)
;   sd a1, 8(a4)
;   ret

