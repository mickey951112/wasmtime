test vcode
target aarch64

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = iadd.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  add x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret


function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = isub.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  sub x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = imul.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  madd x0, x0, x1, xzr
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = umulhi.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  umulh x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = smulhi.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  smulh x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = sdiv.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  sdiv x2, x0, x1
; nextln:  cbz x1, 20
; nextln:  adds xzr, x1, #1
; nextln:  ccmp x0, #1, #nzcv, eq
; nextln:  b.vc 12
; nextln:  udf
; nextln:  udf
; nextln:  mov x0, x2
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 2
  v2 = sdiv.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  movz x2, #2
; nextln:  sdiv x1, x0, x2
; nextln:  cbz x2, 20
; nextln:  adds xzr, x2, #1
; nextln:  ccmp x0, #1, #nzcv, eq
; nextln:  b.vc 12
; nextln:  udf
; nextln:  udf
; nextln:  mov x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = udiv.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  udiv x0, x0, x1
; nextln:  cbnz x1, 8
; nextln:  udf
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64) -> i64 {
block0(v0: i64):
  v1 = iconst.i64 2
  v2 = udiv.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  movz x1, #2
; nextln:  udiv x0, x0, x1
; nextln:  cbnz x1, 8
; nextln:  udf
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = srem.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  sdiv x2, x0, x1
; nextln:  cbnz x1, 8
; nextln:  udf
; nextln:  msub x0, x2, x1, x0
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = urem.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  udiv x2, x0, x1
; nextln:  cbnz x1, 8
; nextln:  udf
; nextln:  msub x0, x2, x1, x0
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret


function %f(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = sdiv.i32 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  sxtw x3, w0
; nextln:  sxtw x2, w1
; nextln:  sdiv x0, x3, x2
; nextln:  cbz x2, 20
; nextln:  adds wzr, w2, #1
; nextln:  ccmp w3, #1, #nzcv, eq
; nextln:  b.vc 12
; nextln:  udf
; nextln:  udf
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i32) -> i32 {
block0(v0: i32):
  v1 = iconst.i32 2
  v2 = sdiv.i32 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  mov x1, x0
; nextln:  movz x0, #2
; nextln:  sxtw x1, w1
; nextln:  sxtw x2, w0
; nextln:  sdiv x0, x1, x2
; nextln:  cbz x2, 20
; nextln:  adds wzr, w2, #1
; nextln:  ccmp w1, #1, #nzcv, eq
; nextln:  b.vc 12
; nextln:  udf
; nextln:  udf
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = udiv.i32 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  mov w0, w0
; nextln:  mov w1, w1
; nextln:  udiv x0, x0, x1
; nextln:  cbnz x1, 8
; nextln:  udf
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i32) -> i32 {
block0(v0: i32):
  v1 = iconst.i32 2
  v2 = udiv.i32 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  movz x1, #2
; nextln:  mov w0, w0
; nextln:  mov w1, w1
; nextln:  udiv x0, x0, x1
; nextln:  cbnz x1, 8
; nextln:  udf
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = srem.i32 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  sxtw x0, w0
; nextln:  sxtw x1, w1
; nextln:  sdiv x2, x0, x1
; nextln:  cbnz x1, 8
; nextln:  udf
; nextln:  msub x0, x2, x1, x0
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i32, i32) -> i32 {
block0(v0: i32, v1: i32):
  v2 = urem.i32 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  mov w0, w0
; nextln:  mov w1, w1
; nextln:  udiv x2, x0, x1
; nextln:  cbnz x1, 8
; nextln:  udf
; nextln:  msub x0, x2, x1, x0
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = band.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  and x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bor.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  orr x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bxor.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  eor x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = band_not.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  bic x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bor_not.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  orn x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bxor_not.i64 v0, v1
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  eon x0, x0, x1
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret

function %f(i64, i64) -> i64 {
block0(v0: i64, v1: i64):
  v2 = bnot.i64 v0
  return v2
}

; check:  stp fp, lr, [sp, #-16]!
; nextln:  mov fp, sp
; nextln:  orn x0, xzr, x0
; nextln:  mov sp, fp
; nextln:  ldp fp, lr, [sp], #16
; nextln:  ret
